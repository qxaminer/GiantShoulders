{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giant Shoulders - Strategic Open Source Discovery Prototype\n",
    "\n",
    "This notebook contains the prototype implementation of Giant Shoulders, an AI-powered strategic discovery system that analyzes the entire GitHub ecosystem to find open source projects perfectly aligned with your career trajectory, learning goals, and networking objectives.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Giant Shoulders transforms how developers discover meaningful contribution opportunities by:\n",
    "- **Strategic Analysis**: Evaluating projects against your career goals and target companies\n",
    "- **Learning Alignment**: Finding projects that match your skill development objectives  \n",
    "- **Contribution Matching**: Identifying issues and opportunities that fit your time and complexity preferences\n",
    "- **Network Building**: Connecting you with maintainers and communities aligned with your professional goals\n",
    "\n",
    "## The Problem We're Solving\n",
    "\n",
    "Developers waste countless hours randomly browsing GitHub hoping to find meaningful projects to contribute to. Most discovery is:\n",
    "- **Random**: No strategic direction or career alignment\n",
    "- **Overwhelming**: Too many options without clear evaluation criteria\n",
    "- **Shallow**: Surface-level browsing without deep project analysis\n",
    "- **Disconnected**: No consideration of long-term career or learning objectives\n",
    "\n",
    "## Prototype Components\n",
    "\n",
    "1. **GitHub Strategic Scanner** - Intelligently discover projects based on strategic criteria\n",
    "2. **Strategic Project Analyzer** - Deep analysis of projects against career goals\n",
    "3. **Contribution Opportunity Mapper** - Identify specific contribution opportunities\n",
    "4. **Strategic Decision Framework** - Provide context and tradeoffs for decision-making\n",
    "5. **Action Plan Generator** - Create specific next steps for each opportunity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path for local imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "# Other utilities\n",
    "import httpx\n",
    "import requests\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
    "\n",
    "console = Console()\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strategic discovery data structures defined!\n"
     ]
    }
   ],
   "source": [
    "# Define core data structures for strategic open source discovery\n",
    "@dataclass\n",
    "class GitHubProject:\n",
    "    \"\"\"Represents a GitHub project with strategic analysis data\"\"\"\n",
    "    name: str\n",
    "    owner: str\n",
    "    description: str\n",
    "    url: str\n",
    "    language: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "    issues_count: int\n",
    "    last_updated: str\n",
    "    topics: List[str] = None\n",
    "    contributors_count: Optional[int] = None\n",
    "    license: Optional[str] = None\n",
    "    strategic_score: Optional[float] = None\n",
    "    learning_alignment: Optional[float] = None\n",
    "    contribution_opportunities: List[str] = None\n",
    "    \n",
    "@dataclass\n",
    "class StrategicProfile:\n",
    "    \"\"\"Represents a developer's strategic profile for project discovery\"\"\"\n",
    "    # Professional Profile\n",
    "    current_role: str\n",
    "    experience_level: str  # junior, mid_level, senior, staff, principal\n",
    "    primary_technologies: List[str]\n",
    "    learning_technologies: List[str]\n",
    "    industry_interests: List[str]\n",
    "    \n",
    "    # Career Goals\n",
    "    target_companies: List[str]\n",
    "    target_roles: List[str]\n",
    "    timeline_months: int\n",
    "    \n",
    "    # Contribution Preferences\n",
    "    time_commitment_hours_per_week: int\n",
    "    preferred_languages: List[str]\n",
    "    issue_complexity: List[str]  # good-first-issue, help-wanted, documentation, bug, feature\n",
    "    project_size_preference: str  # startup, established, enterprise\n",
    "\n",
    "@dataclass\n",
    "class ContributionOpportunity:\n",
    "    \"\"\"Represents a specific contribution opportunity with strategic context\"\"\"\n",
    "    project: GitHubProject\n",
    "    opportunity_type: str  # issue, documentation, feature, bug-fix, optimization\n",
    "    title: str\n",
    "    description: str\n",
    "    url: str\n",
    "    difficulty: str  # beginner, intermediate, advanced\n",
    "    strategic_value: float  # 0-1 score for career alignment\n",
    "    learning_value: float   # 0-1 score for skill development\n",
    "    networking_value: float # 0-1 score for professional connections\n",
    "    estimated_hours: int\n",
    "    next_steps: List[str]\n",
    "\n",
    "print(\"‚úÖ Strategic discovery data structures defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Discovery Engine\n",
    "\n",
    "The Research Discovery Engine is responsible for finding relevant research papers and resources based on a user's query. This prototype uses multiple search strategies:\n",
    "\n",
    "1. **Semantic Search** - Using embeddings to find conceptually similar papers\n",
    "2. **Keyword Search** - Traditional keyword-based search\n",
    "3. **Citation Network Analysis** - Following citation relationships\n",
    "4. **Cross-reference Discovery** - Finding papers that cite common sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResearchQuery' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mResearchDiscoveryEngine\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Prototype research discovery engine that finds relevant papers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    using multiple search strategies.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mResearchDiscoveryEngine\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         papers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m             {\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Paper \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(max_results, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# Mock 3 results\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         ]\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m papers\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdiscover_papers\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[43mResearchQuery\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[ResearchPaper]:\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Main discovery method that combines multiple search strategies\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     console\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Discovering papers for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResearchQuery' is not defined"
     ]
    }
   ],
   "source": [
    "class ResearchDiscoveryEngine:\n",
    "    \"\"\"\n",
    "    Prototype research discovery engine that finds relevant papers\n",
    "    using multiple search strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.search_apis = {\n",
    "            'arxiv': 'http://export.arxiv.org/api/query',\n",
    "            'semantic_scholar': 'https://api.semanticscholar.org/graph/v1/paper/search',\n",
    "        }\n",
    "        \n",
    "    async def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search arXiv for papers matching the query\"\"\"\n",
    "        params = {\n",
    "            'search_query': query,\n",
    "            'start': 0,\n",
    "            'max_results': max_results,\n",
    "            'sortBy': 'relevance',\n",
    "            'sortOrder': 'descending'\n",
    "        }\n",
    "        \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(self.search_apis['arxiv'], params=params)\n",
    "            \n",
    "        # Parse XML response (simplified for prototype)\n",
    "        papers = []\n",
    "        if response.status_code == 200:\n",
    "            # In a real implementation, we'd parse the XML properly\n",
    "            # For now, return mock data structure\n",
    "            papers = [\n",
    "                {\n",
    "                    'title': f'Sample Paper {i+1} for: {query}',\n",
    "                    'authors': [f'Author {i+1}A', f'Author {i+1}B'],\n",
    "                    'abstract': f'This is a sample abstract for paper {i+1} related to {query}...',\n",
    "                    'url': f'https://arxiv.org/abs/2024.{i+1:04d}',\n",
    "                    'publication_date': '2024-01-01'\n",
    "                }\n",
    "                for i in range(min(max_results, 3))  # Mock 3 results\n",
    "            ]\n",
    "            \n",
    "        return papers\n",
    "    \n",
    "    async def discover_papers(self, query: ResearchQuery) -> List[ResearchPaper]:\n",
    "        \"\"\"Main discovery method that combines multiple search strategies\"\"\"\n",
    "        console.print(f\"üîç Discovering papers for: {query.query}\")\n",
    "        \n",
    "        # Search different sources\n",
    "        arxiv_papers = await self.search_arxiv(query.query)\n",
    "        \n",
    "        # Convert to ResearchPaper objects\n",
    "        papers = []\n",
    "        for paper_data in arxiv_papers:\n",
    "            paper = ResearchPaper(\n",
    "                title=paper_data['title'],\n",
    "                authors=paper_data['authors'],\n",
    "                abstract=paper_data['abstract'],\n",
    "                url=paper_data['url'],\n",
    "                publication_date=paper_data.get('publication_date')\n",
    "            )\n",
    "            papers.append(paper)\n",
    "            \n",
    "        console.print(f\"‚úÖ Found {len(papers)} papers\")\n",
    "        return papers\n",
    "\n",
    "# Test the discovery engine\n",
    "discovery_engine = ResearchDiscoveryEngine()\n",
    "print(\"‚úÖ Research Discovery Engine initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Extraction Engine\n",
    "\n",
    "The Knowledge Extraction Engine analyzes research papers to extract key insights, methodologies, and findings. It uses LLM-powered analysis to understand the content and structure knowledge for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeExtractionEngine:\n",
    "    \"\"\"\n",
    "    Extracts key insights and knowledge from research papers\n",
    "    using LLM-powered analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In a real implementation, we'd initialize an LLM here\n",
    "        # For prototype, we'll simulate the extraction\n",
    "        self.extraction_prompt = \"\"\"\n",
    "        Analyze the following research paper and extract:\n",
    "        1. Key insights and findings\n",
    "        2. Methodologies used\n",
    "        3. Main contributions\n",
    "        4. Limitations mentioned\n",
    "        5. Future work suggestions\n",
    "        \n",
    "        Paper Title: {title}\n",
    "        Abstract: {abstract}\n",
    "        \n",
    "        Return the analysis in JSON format.\n",
    "        \"\"\"\n",
    "        \n",
    "    def extract_insights(self, paper: ResearchPaper) -> Dict[str, Any]:\n",
    "        \"\"\"Extract key insights from a research paper\"\"\"\n",
    "        console.print(f\"üß† Extracting insights from: {paper.title[:50]}...\")\n",
    "        \n",
    "        # Simulate LLM-powered extraction\n",
    "        insights = {\n",
    "            \"key_findings\": [\n",
    "                \"Novel approach to the problem domain\",\n",
    "                \"Significant improvement over baseline methods\",\n",
    "                \"Identifies important limitations in current approaches\"\n",
    "            ],\n",
    "            \"methodologies\": [\n",
    "                \"Machine learning approach\",\n",
    "                \"Experimental validation\",\n",
    "                \"Comparative analysis\"\n",
    "            ],\n",
    "            \"contributions\": [\n",
    "                \"New theoretical framework\",\n",
    "                \"Improved performance metrics\",\n",
    "                \"Open source implementation\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Limited dataset size\",\n",
    "                \"Computational complexity\",\n",
    "                \"Generalization concerns\"\n",
    "            ],\n",
    "            \"future_work\": [\n",
    "                \"Larger scale experiments\",\n",
    "                \"Real-world deployment\",\n",
    "                \"Cross-domain validation\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Update paper with extracted insights\n",
    "        paper.key_insights = insights[\"key_findings\"]\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def batch_extract(self, papers: List[ResearchPaper]) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Extract insights from multiple papers\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        with Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            console=console\n",
    "        ) as progress:\n",
    "            task = progress.add_task(\"Extracting insights...\", total=len(papers))\n",
    "            \n",
    "            for paper in papers:\n",
    "                insights = self.extract_insights(paper)\n",
    "                results[paper.title] = insights\n",
    "                progress.advance(task)\n",
    "                \n",
    "        return results\n",
    "\n",
    "# Initialize the extraction engine\n",
    "extraction_engine = KnowledgeExtractionEngine()\n",
    "print(\"‚úÖ Knowledge Extraction Engine initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Integration\n",
    "\n",
    "Now let's integrate everything into a LangGraph workflow that orchestrates the research discovery and knowledge extraction process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our LangGraph workflow\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    query: ResearchQuery\n",
    "    discovered_papers: List[ResearchPaper]\n",
    "    extracted_insights: Dict[str, Dict[str, Any]]\n",
    "    connections: List[KnowledgeConnection]\n",
    "    synthesis: Optional[str]\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Define workflow nodes\n",
    "async def discover_research_node(state: ResearchState):\n",
    "    \"\"\"Node that discovers relevant research papers\"\"\"\n",
    "    console.print(\"üîç [bold blue]Starting research discovery...[/bold blue]\")\n",
    "    \n",
    "    papers = await discovery_engine.discover_papers(state[\"query\"])\n",
    "    \n",
    "    return {\n",
    "        \"discovered_papers\": papers,\n",
    "        \"messages\": [HumanMessage(content=f\"Discovered {len(papers)} papers\")]\n",
    "    }\n",
    "\n",
    "async def extract_knowledge_node(state: ResearchState):\n",
    "    \"\"\"Node that extracts knowledge from discovered papers\"\"\"\n",
    "    console.print(\"üß† [bold green]Extracting knowledge from papers...[/bold green]\")\n",
    "    \n",
    "    papers = state[\"discovered_papers\"]\n",
    "    insights = extraction_engine.batch_extract(papers)\n",
    "    \n",
    "    return {\n",
    "        \"extracted_insights\": insights,\n",
    "        \"messages\": [HumanMessage(content=f\"Extracted insights from {len(insights)} papers\")]\n",
    "    }\n",
    "\n",
    "async def synthesize_knowledge_node(state: ResearchState):\n",
    "    \"\"\"Node that synthesizes knowledge into a coherent summary\"\"\"\n",
    "    console.print(\"üìù [bold yellow]Synthesizing knowledge...[/bold yellow]\")\n",
    "    \n",
    "    # Create a simple synthesis (in real implementation, this would use LLM)\n",
    "    papers = state[\"discovered_papers\"]\n",
    "    insights = state[\"extracted_insights\"]\n",
    "    \n",
    "    synthesis = f\"\"\"\n",
    "    # Research Synthesis for: {state['query'].query}\n",
    "    \n",
    "    ## Overview\n",
    "    Found {len(papers)} relevant papers in the domain of {state['query'].domain}.\n",
    "    \n",
    "    ## Key Themes\n",
    "    - Novel methodological approaches\n",
    "    - Performance improvements over baselines  \n",
    "    - Identification of current limitations\n",
    "    \n",
    "    ## Research Opportunities\n",
    "    - Larger scale validation studies\n",
    "    - Cross-domain applications\n",
    "    - Real-world deployment considerations\n",
    "    \n",
    "    ## Next Steps\n",
    "    Based on the analysis, researchers should focus on addressing scalability \n",
    "    and generalization challenges while exploring cross-domain applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"synthesis\": synthesis,\n",
    "        \"messages\": [HumanMessage(content=\"Knowledge synthesis completed\")]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow nodes defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangGraph workflow\n",
    "def create_research_workflow():\n",
    "    \"\"\"Create the Giant Shoulders research workflow\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"discover\", discover_research_node)\n",
    "    workflow.add_node(\"extract\", extract_knowledge_node) \n",
    "    workflow.add_node(\"synthesize\", synthesize_knowledge_node)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.set_entry_point(\"discover\")\n",
    "    workflow.add_edge(\"discover\", \"extract\")\n",
    "    workflow.add_edge(\"extract\", \"synthesize\")\n",
    "    workflow.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create the workflow\n",
    "research_workflow = create_research_workflow()\n",
    "console.print(\"‚úÖ [bold green]Giant Shoulders research workflow created![/bold green]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Running the Giant Shoulders Prototype\n",
    "\n",
    "Let's test our prototype with a sample research query!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample research query\n",
    "sample_query = ResearchQuery(\n",
    "    query=\"large language models for code generation\",\n",
    "    domain=\"artificial intelligence\",\n",
    "    research_goals=[\n",
    "        \"Understand current state-of-the-art in LLM code generation\",\n",
    "        \"Identify limitations and challenges\",\n",
    "        \"Find opportunities for novel contributions\"\n",
    "    ],\n",
    "    existing_knowledge=\"Basic understanding of transformers and neural language models\"\n",
    ")\n",
    "\n",
    "# Display the query\n",
    "console.print(Panel.fit(\n",
    "    f\"[bold]Research Query:[/bold]\\n\"\n",
    "    f\"üîç Query: {sample_query.query}\\n\"\n",
    "    f\"üè∑Ô∏è Domain: {sample_query.domain}\\n\"\n",
    "    f\"üéØ Goals: {', '.join(sample_query.research_goals[:2])}...\",\n",
    "    title=\"Giant Shoulders Demo\",\n",
    "    border_style=\"blue\"\n",
    "))\n",
    "\n",
    "print(\"‚úÖ Sample research query created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Giant Shoulders workflow\n",
    "async def run_demo():\n",
    "    \"\"\"Run the complete Giant Shoulders demo\"\"\"\n",
    "    \n",
    "    console.print(\"\\nüöÄ [bold magenta]Starting Giant Shoulders Research Assistant...[/bold magenta]\\n\")\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = {\n",
    "        \"query\": sample_query,\n",
    "        \"discovered_papers\": [],\n",
    "        \"extracted_insights\": {},\n",
    "        \"connections\": [],\n",
    "        \"synthesis\": None,\n",
    "        \"messages\": []\n",
    "    }\n",
    "    \n",
    "    # Run the workflow\n",
    "    result = await research_workflow.ainvoke(initial_state)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Note: In Jupyter, we need to use asyncio to run async functions\n",
    "import asyncio\n",
    "\n",
    "# Run the demo\n",
    "try:\n",
    "    result = await run_demo()\n",
    "    console.print(\"\\n‚úÖ [bold green]Giant Shoulders workflow completed successfully![/bold green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"\\n‚ùå [bold red]Error running workflow: {e}[/bold red]\")\n",
    "    # For demo purposes, let's create mock results\n",
    "    result = {\n",
    "        \"query\": sample_query,\n",
    "        \"discovered_papers\": [\n",
    "            ResearchPaper(\n",
    "                title=\"Sample Paper 1 for: large language models for code generation\",\n",
    "                authors=[\"Author 1A\", \"Author 1B\"],\n",
    "                abstract=\"This is a sample abstract for paper 1 related to large language models for code generation...\",\n",
    "                url=\"https://arxiv.org/abs/2024.0001\"\n",
    "            )\n",
    "        ],\n",
    "        \"synthesis\": \"Mock synthesis completed for demo purposes\"\n",
    "    }\n",
    "    console.print(\"üìù [italic]Using mock results for demo[/italic]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a nice format\n",
    "def display_results(result):\n",
    "    \"\"\"Display the research results in a formatted way\"\"\"\n",
    "    \n",
    "    # Display discovered papers\n",
    "    if \"discovered_papers\" in result and result[\"discovered_papers\"]:\n",
    "        papers_table = Table(title=\"üìö Discovered Research Papers\")\n",
    "        papers_table.add_column(\"Title\", style=\"cyan\", no_wrap=False)\n",
    "        papers_table.add_column(\"Authors\", style=\"magenta\")\n",
    "        papers_table.add_column(\"URL\", style=\"blue\")\n",
    "        \n",
    "        for paper in result[\"discovered_papers\"]:\n",
    "            papers_table.add_row(\n",
    "                paper.title[:60] + \"...\" if len(paper.title) > 60 else paper.title,\n",
    "                \", \".join(paper.authors[:2]) + (\"...\" if len(paper.authors) > 2 else \"\"),\n",
    "                paper.url\n",
    "            )\n",
    "        \n",
    "        console.print(papers_table)\n",
    "    \n",
    "    # Display synthesis\n",
    "    if \"synthesis\" in result and result[\"synthesis\"]:\n",
    "        console.print(Panel(\n",
    "            result[\"synthesis\"],\n",
    "            title=\"üìù Research Synthesis\",\n",
    "            border_style=\"green\"\n",
    "        ))\n",
    "    \n",
    "    console.print(\"\\nüéâ [bold green]Giant Shoulders prototype demo complete![/bold green]\")\n",
    "\n",
    "# Display the results\n",
    "display_results(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Future Enhancements\n",
    "\n",
    "This prototype demonstrates the core concepts of Giant Shoulders. Here are the key areas for future development:\n",
    "\n",
    "### üîß Technical Improvements\n",
    "1. **Real API Integration** - Connect to actual research databases (arXiv, Semantic Scholar, PubMed)\n",
    "2. **LLM Integration** - Add real language model for knowledge extraction and synthesis\n",
    "3. **Vector Embeddings** - Implement semantic search using embeddings\n",
    "4. **Citation Analysis** - Build citation network analysis capabilities\n",
    "\n",
    "### üöÄ Feature Enhancements\n",
    "1. **Interactive Interface** - Build web UI for researchers\n",
    "2. **Collaboration Tools** - Multi-user research projects\n",
    "3. **Export Capabilities** - Generate literature reviews, bibliographies\n",
    "4. **Real-time Updates** - Monitor new papers in research areas\n",
    "\n",
    "### üìä Advanced Analytics\n",
    "1. **Trend Analysis** - Identify emerging research directions\n",
    "2. **Gap Detection** - Automatically identify research gaps\n",
    "3. **Impact Prediction** - Predict potential impact of research directions\n",
    "4. **Collaboration Recommendations** - Suggest potential collaborators\n",
    "\n",
    "### üîí Production Considerations\n",
    "1. **Rate Limiting** - Respect API limits\n",
    "2. **Caching** - Cache results for efficiency\n",
    "3. **Authentication** - User management and API keys\n",
    "4. **Scalability** - Handle large-scale research queries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
